{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use TensorFlow to build a model which can classify images of hand-drawn numbers (from 0 to 9). In particular, we will create a standard neural network with dropout and ultilizing mini-batches. \n",
    "\n",
    "We will train/test using the MNIST dataset. I assume basic familiarity with MNIST and also with commonly-used deep learning terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model + Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "MNIST = input_data.read_data_sets('data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's decide the topology of our network using a list called 'layer_dims'. This list will contain the number of neurons in each layer. We will work with layer_dims when creating our model, so I believe that defining it early is useful for learning purposes.\n",
    "\n",
    "We will have 28x28 input features, 500 neurons in the first layer, 250 in the second, 125 in the third, and we have 10 output classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [784, 500, 250, 125, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create our placeholders, which will be filled with values once we run our tensorflow session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None, layer_dims[0]]) #input\n",
    "y = tf.placeholder('float', [None, layer_dims[-1]]) #output\n",
    "probability = tf.placeholder('float') #dropout probability of keeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function 'initialize()' to initialize the weights and biases given our network topology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dims):\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters['W' + str(l)] = tf.Variable(tf.random_normal([layer_dims[l-1], layer_dims[l]], stddev = 0.1))\n",
    "        parameters['b' + str(l)] = tf.Variable(tf.random_normal([layer_dims[l]]))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function 'forward()' to complete the forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, parameters, probability, layer_dims):\n",
    "\n",
    "    layers = {}\n",
    "    \n",
    "    for l in range(1, len(layer_dims) - 1):\n",
    "        x = tf.nn.relu(tf.add(tf.matmul(x, parameters['W' + str(l)]), parameters['b' + str(l)]))\n",
    "        layers['layer' + str(l)] = x\n",
    "        \n",
    "    x = tf.nn.relu(tf.add(tf.matmul(layers['layer' + str(len(layer_dims) - 3)], parameters['W' + str(len(layer_dims) - 2)]),\\\n",
    "        parameters['b' + str(len(layer_dims) - 2)]))\n",
    "    layers['layer' + str(len(layer_dims) - 2)] = tf.nn.dropout(x, probability) \n",
    "    \n",
    "    return (tf.matmul(layers['layer' + str(len(layer_dims) - 2)], parameters['W' + str(len(layer_dims) - 1)])\\\n",
    "            + parameters['b' + str(len(layer_dims) - 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare to optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize(layer_dims)\n",
    "predictions = forward(x, parameters, probability, layer_dims)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = predictions, labels = y)) \n",
    "opt = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost) \n",
    "result = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))    \n",
    "accuracy = tf.reduce_mean(tf.cast(result, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we optimize using mini-batches of 100 and 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1\n",
      "cost: 0.2843735211850567\n",
      "training accuracy: 0.96\n",
      "testing accuracy: 0.9548\n",
      "epoch number: 2\n",
      "cost: 0.09861007609997283\n",
      "training accuracy: 0.98\n",
      "testing accuracy: 0.9678\n",
      "epoch number: 3\n",
      "cost: 0.060979520800438794\n",
      "training accuracy: 0.98\n",
      "testing accuracy: 0.9702\n",
      "epoch number: 4\n",
      "cost: 0.041437779547731306\n",
      "training accuracy: 0.99\n",
      "testing accuracy: 0.9726\n",
      "epoch number: 5\n",
      "cost: 0.029306886420288884\n",
      "training accuracy: 0.98\n",
      "testing accuracy: 0.9752\n",
      "epoch number: 6\n",
      "cost: 0.021961649753449653\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9781\n",
      "epoch number: 7\n",
      "cost: 0.0164081841009796\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9801\n",
      "epoch number: 8\n",
      "cost: 0.011004307649246502\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9807\n",
      "epoch number: 9\n",
      "cost: 0.010923360799682666\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9786\n",
      "epoch number: 10\n",
      "cost: 0.00970885508689786\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9805\n",
      "epoch number: 11\n",
      "cost: 0.00692673334892871\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9814\n",
      "epoch number: 12\n",
      "cost: 0.005901485458872974\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9796\n",
      "epoch number: 13\n",
      "cost: 0.005588487494844065\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9779\n",
      "epoch number: 14\n",
      "cost: 0.004955107396032293\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9797\n",
      "epoch number: 15\n",
      "cost: 0.0039061336538113458\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.9796\n"
     ]
    }
   ],
   "source": [
    "start = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(start)\n",
    "\n",
    "for epoch in range(15):\n",
    "    \n",
    "    batches = int(MNIST.train.num_examples/100)\n",
    "    average = 0.0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        x_, y_ = MNIST.train.next_batch(100)\n",
    "        sess.run(opt, feed_dict = {x: x_, y: y_, probability: 0.5})\n",
    "        average = average + sess.run(cost, feed_dict = {x: x_, y: y_, probability: 1.0})\n",
    "        \n",
    "    average = average/batches\n",
    "\n",
    "    print('epoch number: ' + str(epoch + 1))\n",
    "    print('cost: ' + str(average))\n",
    "    train_accuracy = sess.run(accuracy, feed_dict = {x: x_, y: y_, probability: 1.0})\n",
    "    print('training accuracy: ' + str(train_accuracy))\n",
    "    test_accuracy = sess.run(accuracy, feed_dict = {x: MNIST.test.images, y: MNIST.test.labels, probability: 1.0})\n",
    "    print('testing accuracy: ' + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is 97.96% accurate for the training set, which is decent for a standard (e.g. non-convolutional) neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a Single Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an optimized model, let's test it on a custom image. I've drawn a \"3\" and saved it as a jpeg titled '0test3.jpg'. Note that this image follows the same format of the MNIST dataset (28x28, greyscale, white text on a black background). Also note that the image must be normalized and reshaped before we predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mpimg.imread('0test3.jpg')\n",
    "plt.imshow(img, cmap = 'gray')\n",
    "img = img/255 #normalization\n",
    "img = np.reshape(img, (1, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the model the image, softmax the output, and then take the argmax to get a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "img_predict = sess.run(predictions, feed_dict = {x: img, probability: 1})\n",
    "img_predict = tf.nn.softmax(img_predict, axis = -1)\n",
    "arg_max = tf.argmax(input=img_predict, axis=1)\n",
    "print(sess.run(arg_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicted a '3', which is correct.\n",
    "\n",
    "As a quick aside, let's see what happens when the input image doesn't follow the format of the MNIST dataset. In particular, what happens if we predict the same image but with inverted colors, so that the text is black and the background is white?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2 = mpimg.imread('0test2.jpg')\n",
    "plt.imshow(img2, cmap = 'gray')\n",
    "img2 = img2/255\n",
    "img2 = np.reshape(img2, (1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "img_predict2 = sess.run(predictions, feed_dict = {x: img2, probability: 1})\n",
    "img_predict2 = tf.nn.softmax(img_predict2, axis = -1)\n",
    "arg_max = tf.argmax(input=img_predict2, axis=1)\n",
    "print(sess.run(arg_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model incorrectly predicted '0' for the inverted image. \n",
    "\n",
    "If we want correct predictions for images which consist of black text on a white background, we must train the model with images following this format. One simple way to do this would be to invert every image in the MNIST dataset (keeping the corresponding labels the same) and then retrain the model. This would be an example of data augmentation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
