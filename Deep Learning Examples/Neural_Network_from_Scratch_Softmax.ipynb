{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (Softmax) from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, I implement a neural network from scratch using python, numpy, and mathematics. While deep learning frameworks allow users to ignore many of the technical details of neural networks, an understanding of the mathematics behind deep learning could allow for an increased level of intuition on the subject. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I define a few activation functions and their derivatives. I will use ReLU activation for hidden layers and Softmax activation for the output layer, but I also wish to define a few other commonly-used functions.\n",
    "\n",
    "I begin with the commonly-used ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(0,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also exists a \"Leaky\" ReLU, which is used to prevent the activation from \"dying.\"\n",
    "\n",
    "Consider inputs with large negative bias: the standard ReLU will consistently return 0. Thus, this ReLU neuron will not be contributing to the network through updating weights. The Leaky ReLU attempts to fix this problem. \n",
    "\n",
    "Note: the 0.01 term in the l_relu function could itself be a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_relu(X):\n",
    "    if X < 0:\n",
    "        return X*0.01\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sigmoid activation is commonly used in the output layer of a two-class logistic regression network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1.0/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tanh activation is a rescaled sigmoid, allowing for stronger gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(X):\n",
    "    return np.tanh(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Softmax activation is commonly used in the output layer of multi-class classification networks. For the purpose of this high-level document, I directly implement its cross-entropy loss function and gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(np.exp(X), axis = 0)\n",
    "\n",
    "def softmax_error(Y,A):\n",
    "    return np.sum(Y * np.log(A))\n",
    "\n",
    "def softmax_grad(Y,A):\n",
    "    return A - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer of the network contains a number of neurons. I use a list num_neurons to represent these numbers. Thus, num_neurons[0] would represent the number of input features, num_neurons[1] would be the number of neurons in the first hidden layer, etc.\n",
    "\n",
    "The function initialize() returns a dictionary containing initialized values of W and b for each layer. W and b are the weight and bias parameters respectively. In particular, we want small (positive or negative) values for the weights, and 0s for the biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(num_neurons):\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, len(num_neurons)):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros([layers_dims[l], 1])\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward propagation step, we use the network input (A), initialized parameters (parameters), and the number of layers (L). We save a cache \"Z_A_back\" of Z values (WA + b) and their activations (Z) to be used in the back propagation step. We also return the activation of the final layer (A). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  forward_prop(A, parameters, L):\n",
    "    \n",
    "    Z_A_back  = {}\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        if l < (L-1):\n",
    "            A = relu(Z)\n",
    "        else:\n",
    "            A = softmax(Z)\n",
    "        Z_A_back['Z' + str(l)] = Z\n",
    "        Z_A_back['A' + str(l)] = A\n",
    "        \n",
    "    return A, Z_A_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we use the activation of the final layer (AL) and the label vector for our examples (Y). We use Y to find the number of examples (m), and then use m, along with the softmax error, to compute the cross-entropy cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costs(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * softmax_error(Y, AL)\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs are network input (X), the label vector (Y), the activation of the final layer (AL), the back prop cache (Z_A_back), the weight/bias parameters (parameters), and the number of layers (L). \n",
    "\n",
    "The function back_prop() calculates the gradient for each parameter and stores them in a dictionary (grads), which is returned. This involves using the softmax gradient for the first step (last layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(X, Y, AL, Z_A_back, parameters, L):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    grads = {}\n",
    "    grad_Z =  (1/m) * softmax_grad(Y, AL)\n",
    "    \n",
    "    for l in reversed(range(1, L-1)):\n",
    "        grads['W' + str(l+1)] = np.dot(grad_Z, Z_A_back['A' + str(l)].T)\n",
    "        grads['b' + str(l+1)] = np.sum(grad_Z, axis=1, keepdims=True)        \n",
    "        grad_A = np.dot(parameters['W' + str(l+1)].T, grad_Z)\n",
    "        grad_A[Z_A_back['Z' + str(l)] <= 0] = 0 #Note: this step, along with the next, correspond to the back prop of ReLU \n",
    "        grad_Z = grad_A\n",
    "\n",
    "    grads['W1'] = np.dot(grad_Z, X.T)\n",
    "    grads['b1'] = np.sum(grad_Z, axis=1, keepdims=True)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the Parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs are the parameters dictionary (parameters), the gradient dictionary (grads), the learning rate (rate), and the number of non-input laters (L).\n",
    "\n",
    "The function update_parameters() uses gradient descent to update the weight and bias parameters. It returns the updated parameters dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, rate, L):\n",
    "    for l in reversed(range(1, L-1)):\n",
    "        parameters['W' + str(l)] -= rate * grads['W' + str(l)]\n",
    "        parameters['b' + str(l)] -= rate * grads['b' + str(l)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network model can now be created by using the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Model(X, Y, X_test, Y_test, num_neurons, iterations, rate, L):\n",
    "    \n",
    "    parameters = initialize(num_neurons)\n",
    "    training_costs = []\n",
    "    testing_costs = []\n",
    "    \n",
    "    for iteration in range(iterations+1):\n",
    "        AL, caches = forward_prop(X, parameters, L)\n",
    "        cost = costs(AL, Y)\n",
    "        grads = back_prop(X, Y, AL, Z_A_back, parameters, L)\n",
    "        parameters = update_parameters(parameters, grads, rate, L)\n",
    "        AL_test, _ = forward_prop(X_test, parameters, L)\n",
    "        cost_test = costs(AL_test, Y_test)\n",
    "        training_costs.append(cost)\n",
    "        testing_costs.append(cost_test)\n",
    "        if iteration % 100 == 0:\n",
    "            print('Iteration number:', iteration, 'Training Cost:', cost, '& Testing Cost:', cost_test)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions can be made using the computed parameters. \n",
    "\n",
    "Softmax returns an n-by-1 array, where n is the number of categories. The top element corresponds to model-calculated probability of category 0, the second element is the probability for category 1, etc.\n",
    "\n",
    "The function predict() takes the argmax of this array to predict the category with the greatest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "\n",
    "    AL, _ = forward_prop(X, parameters)\n",
    "    predictions = np.argmax(AL, axis = 0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for a dataset to be usable with this model, the data must follow a specific format. In particular, X must be an n_x by m matrix, where n_x is the number of features and m is the number of data points. Further, Y must be an n_y by m matrix, where n_y is the number of classification labels and m is the number of data points. A function such as np.eye() may be useful when converting common label formats to the correct format for this model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
